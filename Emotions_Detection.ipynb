{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "074b5c57-f8b8-4de6-8a46-5fe61d61a1fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-25 14:35:41.157690: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-04-25 14:35:41.266140: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-04-25 14:35:41.266532: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-04-25 14:35:41.280293: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-25 14:35:41.322846: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-04-25 14:35:41.323856: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-25 14:35:42.808329: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras.applications import VGG16\n",
    "import cv2\n",
    "import gc\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd01d7e3-70ab-46de-9f60-5fbc9228028f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir=Path(\"Emotions/train\")\n",
    "test_dir=Path(\"Emotions/test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf89bba-21b8-4254-891b-e888cf6b31bc",
   "metadata": {},
   "source": [
    "This cell saves the paths to the images of each emotion as a value to a key(which is the emotion name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "512a1837-38f5-4908-b635-a91c88a9468e",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions_train_images_dict={\n",
    "    'angry': list(train_dir.glob(\"angry/*.jpg\")),\n",
    "    'disgust':list(train_dir.glob(\"disgust/*.jpg\")),\n",
    "    'fear':list(train_dir.glob(\"fear/*.jpg\")),\n",
    "    'happy':list(train_dir.glob(\"happy/*.jpg\")),\n",
    "    'neutral':list(train_dir.glob(\"neutral/*.jpg\")),\n",
    "    'sad':list(train_dir.glob(\"sad/*.jpg\")),\n",
    "    'surprise':list(train_dir.glob(\"surprise/*.jpg\")),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d2675c5-0d26-4f8f-896d-e4d92683002e",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions_test_images_dict={\n",
    "    'angry': list(test_dir.glob(\"angry/*.jpg\")),\n",
    "    'disgust':list(test_dir.glob(\"disgust/*.jpg\")),\n",
    "    'fear':list(test_dir.glob(\"fear/*.jpg\")),\n",
    "    'happy':list(test_dir.glob(\"happy/*.jpg\")),\n",
    "    'neutral':list(test_dir.glob(\"neutral/*.jpg\")),\n",
    "    'sad':list(test_dir.glob(\"sad/*.jpg\")),\n",
    "    'surprise':list(test_dir.glob(\"surprise/*.jpg\")),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c59c74c6-1d63-407e-a1ce-5ce6056ac575",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions_labels_dict={\n",
    "    'angry': 0,\n",
    "    'disgust':1,\n",
    "    'fear':2,\n",
    "    'happy':3,\n",
    "    'neutral':4,\n",
    "    'sad':5,\n",
    "    'surprise':6,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b60b8de0-0231-4a29-b954-4cc8013e9c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Preallocate arrays with float16 dtype (saves 4x memory vs float32)\n",
    "x_train = np.empty((7000, 224, 224, 3), dtype='float16')\n",
    "y_train = np.empty((7000,), dtype='uint8')  # Assuming integer labels\n",
    "\n",
    "#you can add the test data(in our case it's not so important, we can perform small tests manually)\n",
    "#x_test = np.empty((len(emotions_train_images_dict[emotion_name]), 224, 224, 3), dtype='float16')\n",
    "#y_test = np.empty((len(emotions_train_images_dict[emotion_name]),), dtype='uint8')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f658ed30-2be0-44ea-b1cf-57740f35d208",
   "metadata": {},
   "source": [
    "This cell is supposed to fill the x_train and y_train arrays with the preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4579e168-a219-46bc-8997-64718a9377cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill arrays incrementally\n",
    "idx = 0\n",
    "RGB_MEAN = np.array([123.68, 116.779, 103.939], dtype=np.float16)\n",
    "for emotion_name, images in emotions_train_images_dict.items():\n",
    "    for image in images[:1070]:\n",
    "        # Load image as uint8 (OpenCV default)\n",
    "        img = cv2.imread(str(image))  # Shape: (H, W, 3), dtype: uint8        \n",
    "            # Resize and convert color\n",
    "        img = cv2.resize(img, (224, 224))\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Still uint8\n",
    "            \n",
    "        \n",
    "        img = img.astype(np.float16)\n",
    "    \n",
    "        img -= RGB_MEAN\n",
    "        x_train[idx] = img\n",
    "        y_train[idx] = emotions_labels_dict[emotion_name]\n",
    "        idx += 1\n",
    "x_train = x_train[:idx]\n",
    "y_train = y_train[:idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85cbaabf-73e2-4a42-8d67-5cc9671473fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = VGG16(\n",
    "    weights='imagenet',\n",
    "    include_top=False,\n",
    "    input_shape=(224, 224, 3)\n",
    ")\n",
    "base_model.trainable = False  # Freeze layers\n",
    "\n",
    "\n",
    "# Clone model and convert to float16\n",
    "pretrained_model = tf.keras.models.clone_model(base_model)\n",
    "pretrained_model.set_weights([w.astype('float16') for w in base_model.get_weights()])\n",
    "\n",
    "# Set float16 policy for all layers (critical for inference)\n",
    "for layer in pretrained_model.layers:\n",
    "    layer._dtype_policy = tf.keras.mixed_precision.Policy('float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5a18185-e298-47f2-870f-d2c6e84ee402",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    layers.Input((224,224,3)),\n",
    "    pretrained_model,          # Add frozen VGGFace\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(7, activation='softmax')  # 7 emotion classes\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d811cbda-8523-4fbd-afab-cf9c11d483f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',  # For integer labels (y_train/y_test)\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "985153ee-d2c8-4c53-b09e-a400053d6cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-25 14:38:55.857271: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 4128079872 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-25 14:43:23.846318: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 411041792 exceeds 10% of free system memory.\n",
      "2025-04-25 14:43:24.208561: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 411041792 exceeds 10% of free system memory.\n",
      "2025-04-25 14:43:25.503626: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 102760448 exceeds 10% of free system memory.\n",
      "2025-04-25 14:43:25.611287: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 205520896 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "215/215 [==============================] - 1758s 8s/step - loss: 2.1373 - accuracy: 0.2132\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 1741s 8s/step - loss: 1.7896 - accuracy: 0.2760\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 1916s 9s/step - loss: 1.7382 - accuracy: 0.3044\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 1969s 9s/step - loss: 1.6885 - accuracy: 0.3361\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 1979s 9s/step - loss: 1.6552 - accuracy: 0.3509\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    x_train, y_train,\n",
    "    epochs=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af91b47-7e3b-484c-87fd-40e0dab1ba92",
   "metadata": {},
   "source": [
    "This cell is just to test the model (you can replace the string 'happy' with any of the emotions and see whther the model can predict it or not)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa469aa6-f020-4885-809e-b537b17522d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 354ms/step\n",
      "happy\n"
     ]
    }
   ],
   "source": [
    "RGB_MEAN = np.array([123.68, 116.779, 103.939], dtype=np.float16)\n",
    "img = cv2.imread(str(emotions_train_images_dict['happy'][0]))  # Shape: (H, W, 3), dtype: uint8        \n",
    "        # Resize and convert color\n",
    "img = cv2.resize(img, (224, 224))\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Still uint8        \n",
    "img = img.astype(np.float16)\n",
    "\n",
    "\n",
    "img -= RGB_MEAN\n",
    "img = np.expand_dims(img, axis=0) \n",
    "preds = model.predict(img)\n",
    "                # Assume preds is a (1, N) array of probabilities\n",
    "label_index = np.argmax(preds[0])\n",
    "idx_to_emotion = {idx: name for name, idx in emotions_labels_dict.items()}\n",
    "emotion_name=idx_to_emotion.get(label_index)\n",
    "label = emotion_name\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287b2ce3-faed-4211-b4c7-7078b47a3242",
   "metadata": {},
   "source": [
    "The following cell is to capture frames from the webcam feed them to the model to predict the emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b0d4209-be05-4e12-9300-a3641cf3e491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 313ms/step\n",
      "1/1 [==============================] - 0s 286ms/step\n",
      "1/1 [==============================] - 0s 285ms/step\n",
      "1/1 [==============================] - 0s 294ms/step\n",
      "1/1 [==============================] - 0s 328ms/step\n",
      "1/1 [==============================] - 0s 288ms/step\n",
      "1/1 [==============================] - 0s 291ms/step\n",
      "1/1 [==============================] - 0s 305ms/step\n",
      "1/1 [==============================] - 0s 285ms/step\n",
      "1/1 [==============================] - 0s 293ms/step\n",
      "1/1 [==============================] - 0s 285ms/step\n",
      "1/1 [==============================] - 0s 305ms/step\n",
      "1/1 [==============================] - 0s 292ms/step\n",
      "1/1 [==============================] - 0s 286ms/step\n",
      "1/1 [==============================] - 0s 287ms/step\n",
      "1/1 [==============================] - 0s 287ms/step\n",
      "1/1 [==============================] - 0s 289ms/step\n",
      "1/1 [==============================] - 0s 307ms/step\n",
      "1/1 [==============================] - 0s 303ms/step\n",
      "1/1 [==============================] - 0s 289ms/step\n",
      "1/1 [==============================] - 0s 285ms/step\n",
      "1/1 [==============================] - 0s 284ms/step\n",
      "1/1 [==============================] - 0s 303ms/step\n",
      "1/1 [==============================] - 0s 288ms/step\n",
      "1/1 [==============================] - 0s 286ms/step\n",
      "1/1 [==============================] - 0s 292ms/step\n",
      "1/1 [==============================] - 0s 311ms/step\n",
      "1/1 [==============================] - 0s 285ms/step\n",
      "1/1 [==============================] - 0s 284ms/step\n",
      "1/1 [==============================] - 0s 284ms/step\n",
      "1/1 [==============================] - 0s 285ms/step\n",
      "1/1 [==============================] - 0s 284ms/step\n",
      "1/1 [==============================] - 0s 285ms/step\n",
      "1/1 [==============================] - 0s 310ms/step\n",
      "1/1 [==============================] - 0s 285ms/step\n",
      "1/1 [==============================] - 0s 284ms/step\n",
      "1/1 [==============================] - 0s 286ms/step\n",
      "1/1 [==============================] - 0s 303ms/step\n",
      "1/1 [==============================] - 0s 293ms/step\n",
      "1/1 [==============================] - 0s 356ms/step\n",
      "1/1 [==============================] - 0s 285ms/step\n",
      "1/1 [==============================] - 0s 309ms/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "VGG16_MEAN_RGB = np.array([123.68, 116.779, 103.939], dtype=np.float16)\n",
    "\n",
    "def prepare_face(face_bgr):\n",
    "    # 1. Resize to (222, 224)\n",
    "    face_resized = cv2.resize(face_bgr, (224, 224))\n",
    "    # 2. Convert BGR -> RGB\n",
    "    face_rgb = cv2.cvtColor(face_resized, cv2.COLOR_BGR2RGB)\n",
    "    # 3. Convert to float32 and subtract VGG16 means\n",
    "    face_normalized = face_rgb.astype(np.float16) - VGG16_MEAN_RGB\n",
    "    # 4. Expand dims to add batch axis\n",
    "    return np.expand_dims(face_normalized, axis=0)\n",
    "\n",
    "def capture_and_classify(device_index=0, cascade_path='haarcascade_frontalface_default.xml'):\n",
    "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + cascade_path)\n",
    "    cap = cv2.VideoCapture(device_index)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Cannot open camera\"); return\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            faces = face_cascade.detectMultiScale(gray, 1.1, 5)\n",
    "\n",
    "            for (x, y, w, h) in faces:\n",
    "                # Extract face Region of interest\n",
    "                face_bgr = frame[y:y+h, x:x+w]\n",
    "\n",
    "                # Prepare for VGG16\n",
    "                face_input = prepare_face(face_bgr)\n",
    "\n",
    "                # Predict with our model\n",
    "                preds = model.predict(face_input)\n",
    "              \n",
    "                label_index = np.argmax(preds[0])\n",
    "                idx_to_emotion = {idx: name for name, idx in emotions_labels_dict.items()}\n",
    "                emotion_name=idx_to_emotion.get(label_index)\n",
    "                label = emotion_name\n",
    "\n",
    "                # Draw\n",
    "                cv2.rectangle(frame, (x, y), (x+w, y+h), (0,255,0), 2)\n",
    "                cv2.putText(frame, label, (x, y-10),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0,255,0), 2)\n",
    "\n",
    "            cv2.imshow('Emotion VGG16', frame)\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "    finally:\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    capture_and_classify()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87141286-99d7-4f59-b222-228e58c8180c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
